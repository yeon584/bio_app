# -*- coding: utf-8 -*-
"""bio_app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17mWKwYBmZH23aJjD6NfabGfwkRJBwuHI
"""

# ìƒë¬¼ ë°ì´í„° ìˆ˜ì§‘
from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import StandardScaler
from matplotlib import pyplot as plt



# Tableone ì´ìš© ë°ì´í„° ì¶œë ¥
pip install tableone

try:
    from tableone import TableOne, load_dataset
except (ModuleNotFoundError, ImportError):

    !pip install tableone
    from tableone import TableOne, load_dataset

data = pd.read_csv('/content/sample_data/bt_dataset_t3.csv')

table4_train = TableOne(data, groupby='Target', pval=True)
print(table4_train)
table4_train.to_csv('./Descriptive_Target.csv')

print(data.columns)
correlation=[]
for j in ['Mean', 'Variance', 'Standard Deviation', 'Entropy', 'Skewness', 'Kurtosis',
          'Contrast', 'Energy', 'ASM', 'Homogeneity', 'Dissimilarity', 'Correlation', 'Coarseness']:

  for i in ['Target']:
    print(j,i)
    X_train_total_del_week=data[[j]]
    X_train_total_del_week_remove = data[[i]]

!pip install umap-learn

from sklearn.manifold import TSNE
import umap
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns

features = ['Mean', 'Variance', 'Standard Deviation', 'Entropy', 'Skewness', 'Kurtosis',
            'Contrast', 'Energy', 'ASM', 'Homogeneity', 'Dissimilarity', 'Correlation','Coarseness']

X = data[features]
y = data['Target']

if X.isnull().sum().sum() > 0:
    print("ê²°ì¸¡ì¹˜ê°€ ìˆì–´ ì œê±°í•©ë‹ˆë‹¤.")
    X = X.dropna()
    y = y[X.index]

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)



# t-SNE
tsne = TSNE(n_components=2, perplexity=30, random_state=42)
X_tsne = tsne.fit_transform(X_scaled)

# UMAP
reducer = umap.UMAP(n_components=2, random_state=42)
X_umap = reducer.fit_transform(X_scaled)

# t-SNE ì‹œê°í™”
plt.figure(figsize=(8,6))
sns.scatterplot(x=X_tsne[:,0], y=X_tsne[:,1], hue=y, palette='Set1', alpha=0.7)
plt.title('t-SNE Visualization')
plt.xlabel('t-SNE 1')
plt.ylabel('t-SNE 2')
plt.legend(title='Target')
plt.grid(True)
plt.show()

# u-MAP ì‹œê°í™”
plt.figure(figsize=(8,6))
sns.scatterplot(x=X_umap[:,0], y=X_umap[:,1], hue=y, palette='Set2', alpha=0.7)
plt.title('UMAP Visualization')
plt.xlabel('UMAP 1')
plt.ylabel('UMAP 2')
plt.legend(title='Target')
plt.grid(True)
plt.show()

# CCA
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.cross_decomposition import CCA
from scipy.stats import pearsonr, spearmanr
import warnings

warnings.filterwarnings("ignore")

# CSV íŒŒì¼ ê²½ë¡œ ì„¤ì • (ì—…ë¡œë“œëœ íŒŒì¼ ê¸°ì¤€)
file_path = "/content/sample_data/bt_dataset_t3.csv"

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
data = pd.read_csv(file_path)

# íƒ€ê²Ÿ ì»¬ëŸ¼ ìë™ íƒìƒ‰
target_col_candidates = [col for col in data.columns if col.lower() == 'target']
if not target_col_candidates:
    raise ValueError("'target' ì»¬ëŸ¼ì´ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤.")
else:
    target_col = target_col_candidates[0]  # ì •í™•íˆëŠ” 'Target'

# ì‚¬ìš©í•  í”¼ì²˜ë“¤ (12ê°€ì§€ íŠ¹ì§•)
selected_features = [
    'Mean', 'Variance', 'Standard Deviation', 'Entropy',
    'Skewness', 'Kurtosis', 'Contrast', 'Energy', 'ASM',
    'Homogeneity', 'Dissimilarity', 'Correlation'
]

# ê²°ê³¼ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
results = []

# ì‹œê°í™” ì¤€ë¹„
fig, axs = plt.subplots(3, 4, figsize=(20, 15))
axs = axs.flatten()

# ê° íŠ¹ì§•ë³„ë¡œ Canonical Correlation ê³„ì‚° ë° ì‹œê°í™”
for idx, feature in enumerate(selected_features):
    try:
        X1 = data[data[target_col] == 0][[feature]].dropna()
        X2 = data[data[target_col] == 1][[feature]].dropna()

        # ë‘ ê·¸ë£¹ì—ì„œ ë™ì¼í•œ ê°œìˆ˜ë§Œí¼ ìƒ˜í”Œ ìœ ì§€ (min ê¸°ì¤€)
        min_len = min(len(X1), len(X2))
        X1 = X1.iloc[:min_len]
        X2 = X2.iloc[:min_len]

        # CCA ì ìš©
        cca = CCA(n_components=1)
        X1_c, X2_c = cca.fit_transform(X1, X2)

        # ìƒê´€ê³„ìˆ˜ ê³„ì‚°
        r_pearson, _ = pearsonr(X1_c[:, 0], X2_c[:, 0])
        r_spearman, _ = spearmanr(X1_c[:, 0], X2_c[:, 0])

        results.append({
            'Feature': feature,
            'Pearson r': r_pearson,
            'Spearman r': r_spearman
        })

        # ì‚°ì ë„ ê·¸ë˜í”„
        ax = axs[idx]
        ax.scatter(X1_c[:, 0], X2_c[:, 0], alpha=0.7)
        ax.set_title(f"{feature}\nPearson r={r_pearson:.2f}, Spearman r={r_spearman:.2f}")
        ax.set_xlabel("Non-Tumor (Group 0)")
        ax.set_ylabel("Tumor (Group 1)")

    except Exception as e:
        print(f"[ERROR] {feature} - CCA ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# ì „ì²´ ê·¸ë˜í”„ ë ˆì´ì•„ì›ƒ ì¡°ì • ë° ì¶œë ¥
plt.tight_layout()
plt.suptitle("Canonical Correlation Analysis (Group 0 vs. Group 1)", fontsize=16, y=1.02)
plt.show()

# ê²°ê³¼ DataFrameìœ¼ë¡œ ì •ë¦¬
results_df = pd.DataFrame(results)
results_df

#ë¨¸ì‹ ëŸ¬ë‹ ì„±ëŠ¥ ë¹„êµ(5ì¢…)
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
data = pd.read_csv('/content/sample_data/bt_dataset_t3.csv')

features = ['Mean', 'Variance', 'Standard Deviation', 'Entropy', 'Skewness', 'Kurtosis',
            'Contrast', 'Energy', 'ASM', 'Homogeneity', 'Dissimilarity', 'Correlation','Coarseness']
X = data[features]
y = data['Target']

# ê²°ì¸¡ì¹˜ ì œê±°
X = X.dropna()
y = y[X.index]

# ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
models = {
    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
    'SVM': SVC(kernel='rbf', probability=True, random_state=42),
    'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),
    'KNN': KNeighborsClassifier(n_neighbors=5),
    'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42)
}

results_ml = []

for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    y_proba = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, 'predict_proba') else None
    acc = accuracy_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_proba) if y_proba is not None else np.nan
    print(f"\n[{name}]")
    print(classification_report(y_test, y_pred))
    print(f"Accuracy: {acc:.4f}, ROC-AUC: {auc:.4f}")
    results_ml.append({'Model': name, 'Accuracy': acc, 'ROC-AUC': auc})

results_ml_df = pd.DataFrame(results_ml)
print("\në¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ:")
print(results_ml_df)



#ë”¥ëŸ¬ë‹ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ(2ì¢…)
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# ë”¥ëŸ¬ë‹ìš© ë°ì´í„° (ì´ë¯¸ ìŠ¤ì¼€ì¼ë§ë¨)
X_train_dl, X_test_dl = X_train_scaled, X_test_scaled
y_train_dl, y_test_dl = y_train.values, y_test.values

# MLP ëª¨ë¸
def build_mlp(input_dim):
    model = keras.Sequential([
        layers.Dense(64, activation='relu', input_shape=(input_dim,)),
        layers.Dense(32, activation='relu'),
        layers.Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])
    return model

# 1D CNN ëª¨ë¸
def build_cnn(input_dim):
    model = keras.Sequential([
        layers.Reshape((input_dim, 1), input_shape=(input_dim,)),
        layers.Conv1D(32, 3, activation='relu'),
        layers.Flatten(),
        layers.Dense(32, activation='relu'),
        layers.Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])
    return model

# í•™ìŠµ ë° í‰ê°€ í•¨ìˆ˜
def train_and_evaluate(model, X_train, y_train, X_test, y_test, epochs=30, batch_size=32):
    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0, validation_split=0.2)
    loss, acc, auc = model.evaluate(X_test, y_test, verbose=0)
    return acc, auc

dl_results = []

# 1. MLP
mlp = build_mlp(X_train_dl.shape[1])
mlp_acc, mlp_auc = train_and_evaluate(mlp, X_train_dl, y_train_dl, X_test_dl, y_test_dl)
dl_results.append({'Model': 'MLP', 'Accuracy': mlp_acc, 'ROC-AUC': mlp_auc})

# 2. 1D CNN
cnn = build_cnn(X_train_dl.shape[1])
cnn_acc, cnn_auc = train_and_evaluate(cnn, X_train_dl, y_train_dl, X_test_dl, y_test_dl)
dl_results.append({'Model': '1D CNN', 'Accuracy': cnn_acc, 'ROC-AUC': cnn_auc})

dl_results_df = pd.DataFrame(dl_results)
print("\në”¥ëŸ¬ë‹ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ:")
print(dl_results_df)

all_results = pd.concat([results_ml_df, dl_results_df], ignore_index=True)
print("\nì „ì²´ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ:")
print(all_results)

# Odd ratio
import pandas as pd
import numpy as np
import statsmodels.api as sm
import scipy.stats as stats

# 1. ë°ì´í„° ë¡œë”©
data = pd.read_csv('/content/sample_data/bt_dataset_t3.csv')

# 2. ì‚¬ìš©í•  feature ë° ì¢…ì† ë³€ìˆ˜ ì„¤ì •
features = ['Mean', 'Variance', 'Standard Deviation', 'Entropy', 'Skewness', 'Kurtosis',
            'Contrast', 'Energy', 'ASM', 'Homogeneity', 'Dissimilarity', 'Correlation', 'Coarseness']
target = 'Target'

# 3. Odds Ratio ê³„ì‚° í•¨ìˆ˜
def odd_ratio(data, features, target):
    print('='*70)
    print(f'{"Feature":<20} {"Odds ratio (CI)":<30} {"p-value":<10}')
    print('-'*70)

    for feature in features:
        try:
            X = sm.add_constant(data[feature])
            y = data[target]

            # Poisson íšŒê·€ë¡œ odds ratio ì¶”ì •
            model = sm.GLM(y, X, family=sm.families.Poisson()).fit()

            coef = model.params[1]
            odds_ratio = np.exp(coef)
            CI = model.conf_int().iloc[1]
            CI_low = np.round(np.exp(CI[0]), 3)
            CI_high = np.round(np.exp(CI[1]), 3)
            p_value = model.pvalues[1]

            print(f'{feature:<20} {odds_ratio:.3f} ({CI_low:.3f} - {CI_high:.3f})   {p_value:.4f}')
        except Exception as e:
            print(f'{feature:<20} ê³„ì‚° ì‹¤íŒ¨ - {e}')

    print('='*70)


# 4. t-test ê³„ì‚° í•¨ìˆ˜
def make_ttest(data, features, target):
    print('='*70)
    print(f'{"Feature":<20} {"t-statistic":<15} {"p-value":<10}')
    print('-'*70)

    for feature in features:
        try:
            group1 = data[data[target] == 0][feature]
            group2 = data[data[target] == 1][feature]

            t_stat, p_value = stats.ttest_ind(group1, group2, nan_policy='omit')
            print(f'{feature:<20} {t_stat:.3f}         {p_value:.4f}')
        except Exception as e:
            print(f'{feature:<20} ê³„ì‚° ì‹¤íŒ¨ - {e}')

    print('='*70)

# 5. ì‹¤í–‰
odd_ratio(data, features, target)
make_ttest(data, features, target)

# SHAP & LIME ë¶„ì„ì„ ìœ„í•œ ì¢…í•© ì½”ë“œ
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")

# SHAP ë° LIME ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import shap
    shap.initjs()
except ImportError:
    print("SHAP ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”: pip install shap")

try:
    import lime
    import lime.lime_tabular
except ImportError:
    print("LIME ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”: pip install lime")

# ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report

# =============================================================================
# 1. ë°ì´í„° ì¤€ë¹„ ë° ëª¨ë¸ í•™ìŠµ
# =============================================================================
def prepare_data_and_models(file_path):
    """ë°ì´í„° ì¤€ë¹„ ë° ì—¬ëŸ¬ ëª¨ë¸ í•™ìŠµ"""
    # ë°ì´í„° ë¡œë”©
    data = pd.read_csv(file_path)

    features = ['Mean', 'Variance', 'Standard Deviation', 'Entropy', 'Skewness', 'Kurtosis',
                'Contrast', 'Energy', 'ASM', 'Homogeneity', 'Dissimilarity', 'Correlation', 'Coarseness']

    X = data[features].dropna()
    y = data.loc[X.index, 'Target']

    # ë°ì´í„° ë¶„í• 
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

    # ìŠ¤ì¼€ì¼ë§ (ì¼ë¶€ ëª¨ë¸ìš©)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # ìŠ¤ì¼€ì¼ëœ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜ (feature names ìœ ì§€)
    X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=features, index=X_train.index)
    X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=features, index=X_test.index)

    # ëª¨ë¸ ë”•ì…”ë„ˆë¦¬
    models = {
        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
        'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
        'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),
        'SVM': SVC(kernel='rbf', probability=True, random_state=42),
        'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=42),
        'KNN': KNeighborsClassifier(n_neighbors=5)
    }

    # ëª¨ë¸ í•™ìŠµ
    trained_models = {}
    for name, model in models.items():
        if name in ['LogisticRegression', 'SVM', 'KNN']:
            # ìŠ¤ì¼€ì¼ëœ ë°ì´í„° ì‚¬ìš©
            model.fit(X_train_scaled, y_train)
            trained_models[name] = {
                'model': model,
                'X_train': X_train_scaled_df,
                'X_test': X_test_scaled_df,
                'scaled': True
            }
        else:
            # ì›ë³¸ ë°ì´í„° ì‚¬ìš©
            model.fit(X_train, y_train)
            trained_models[name] = {
                'model': model,
                'X_train': X_train,
                'X_test': X_test,
                'scaled': False
            }

    return trained_models, y_train, y_test, scaler

# =============================================================================
# 2. SHAP Summary Plot
# =============================================================================
def shap_summary_plot(model, model_name, X, model_type='auto'):
    """SHAP Summary Plot ìƒì„±"""
    print(f'\n=== {model_name} - SHAP Summary Plot ===')

    try:
        # ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ explainer ì„ íƒ
        if isinstance(model, (RandomForestClassifier, GradientBoostingClassifier)):
            explainer = shap.TreeExplainer(model)
            shap_values = explainer.shap_values(X)

            # ì´ì§„ ë¶„ë¥˜ì˜ ê²½ìš° positive classì˜ SHAP values ì‚¬ìš©
            if isinstance(shap_values, list) and len(shap_values) == 2:
                shap_values = shap_values[1]

        elif isinstance(model, (AdaBoostClassifier, BaggingClassifier, SGDClassifier, SVC, KNeighborsClassifier)):
            # ìƒ˜í”Œ í¬ê¸° ì œí•œ (KernelExplainerëŠ” ëŠë¦¼)
            sample_size = min(100, len(X))
            X_sample = X.sample(n=sample_size, random_state=42) if len(X) > sample_size else X

            explainer = shap.KernelExplainer(model.predict_proba, X_sample)
            shap_values = explainer.shap_values(X_sample)

            if isinstance(shap_values, list) and len(shap_values) == 2:
                shap_values = shap_values[1]
            X = X_sample

        else:  # LogisticRegression ë“±
            explainer = shap.LinearExplainer(model, X)
            shap_values = explainer.shap_values(X)

        # Summary plot ìƒì„±
        plt.figure(figsize=(10, 8))
        shap.summary_plot(shap_values, X, show=False)
        plt.title(f'{model_name} - SHAP Summary Plot')
        plt.tight_layout()
        plt.show()

        return explainer, shap_values

    except Exception as e:
        print(f"SHAP Summary Plot ìƒì„± ì¤‘ ì˜¤ë¥˜ ({model_name}): {str(e)}")
        return None, None

# =============================================================================
# 3. SHAP Force Plot
# =============================================================================
def shap_force_plot(explainer, shap_values, X, model_name, instance_idx=0):
    """SHAP Force Plot ìƒì„±"""
    print(f'\n=== {model_name} - SHAP Force Plot (Instance {instance_idx}) ===')

    try:
        # expected_value ì²˜ë¦¬
        if hasattr(explainer, 'expected_value'):
            if isinstance(explainer.expected_value, np.ndarray):
                expected_value = explainer.expected_value[1] if len(explainer.expected_value) > 1 else explainer.expected_value[0]
            else:
                expected_value = explainer.expected_value
        else:
            expected_value = 0

        # SHAP values ì²˜ë¦¬
        if len(shap_values.shape) == 2:
            instance_shap = shap_values[instance_idx, :]
        else:
            instance_shap = shap_values[instance_idx]

        # Force plot ìƒì„± (matplotlib ë²„ì „)
        plt.figure(figsize=(12, 4))
        shap.force_plot(
            expected_value,
            instance_shap,
            X.iloc[instance_idx, :],
            matplotlib=True,
            show=False
        )
        plt.title(f'{model_name} - SHAP Force Plot (Instance {instance_idx})')
        plt.tight_layout()
        plt.show()

    except Exception as e:
        print(f"SHAP Force Plot ìƒì„± ì¤‘ ì˜¤ë¥˜ ({model_name}): {str(e)}")

# =============================================================================
# 4. SHAP Dependency Plot
# =============================================================================
def shap_dependency_plot(explainer, shap_values, X, model_name, feature=None):
    """SHAP Dependency Plot ìƒì„±"""
    print(f'\n=== {model_name} - SHAP Dependency Plot ===')

    try:
        # ê°€ì¥ ì¤‘ìš”í•œ íŠ¹ì„± ì°¾ê¸° (featureê°€ ì§€ì •ë˜ì§€ ì•Šì€ ê²½ìš°)
        if feature is None:
            mean_abs_shap = np.abs(shap_values).mean(axis=0)
            most_important_idx = np.argmax(mean_abs_shap)
            feature = X.columns[most_important_idx]

        print(f'ë¶„ì„ íŠ¹ì„±: {feature}')

        # Dependency plot ìƒì„±
        plt.figure(figsize=(10, 6))
        shap.dependence_plot(feature, shap_values, X, show=False)
        plt.title(f'{model_name} - SHAP Dependency Plot ({feature})')
        plt.tight_layout()
        plt.show()

        return feature

    except Exception as e:
        print(f"SHAP Dependency Plot ìƒì„± ì¤‘ ì˜¤ë¥˜ ({model_name}): {str(e)}")
        return None

# =============================================================================
# 5. SHAP Feature Importance
# =============================================================================
def shap_feature_importance(shap_values, X, model_name, top_n=10):
    """SHAP ê¸°ë°˜ íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„"""
    print(f'\n=== {model_name} - SHAP Feature Importance ===')

    try:
        # í‰ê·  ì ˆëŒ“ê°’ìœ¼ë¡œ íŠ¹ì„± ì¤‘ìš”ë„ ê³„ì‚°
        feature_importance = np.abs(shap_values).mean(axis=0)

        # DataFrameìœ¼ë¡œ ì •ë¦¬
        importance_df = pd.DataFrame({
            'Feature': X.columns,
            'Importance': feature_importance
        }).sort_values('Importance', ascending=False)

        # ì‹œê°í™”
        plt.figure(figsize=(10, 8))
        sns.barplot(data=importance_df.head(top_n), y='Feature', x='Importance')
        plt.title(f'{model_name} - SHAP Feature Importance (Top {top_n})')
        plt.xlabel('Mean |SHAP Value|')
        plt.tight_layout()
        plt.show()

        print(f"Top {top_n} ì¤‘ìš” íŠ¹ì„±:")
        print(importance_df.head(top_n))

        return importance_df

    except Exception as e:
        print(f"SHAP Feature Importance ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ({model_name}): {str(e)}")
        return None

# =============================================================================
# 6. LIME ë¶„ì„
# =============================================================================
def lime_analysis(model, X_train, X_test, y_train, model_name, instance_idx=0, num_features=10):
    """LIME ë¶„ì„"""
    print(f'\n=== {model_name} - LIME Analysis (Instance {instance_idx}) ===')

    try:
        # LIME explainer ìƒì„±
        explainer = lime.lime_tabular.LimeTabularExplainer(
            X_train.values,
            feature_names=X_train.columns,
            class_names=['Non-Tumor', 'Tumor'],
            mode='classification',
            discretize_continuous=True
        )

        # íŠ¹ì • ì¸ìŠ¤í„´ìŠ¤ì— ëŒ€í•œ ì„¤ëª… ìƒì„±
        instance = X_test.iloc[instance_idx].values
        explanation = explainer.explain_instance(
            instance,
            model.predict_proba,
            num_features=num_features
        )

        # ì„¤ëª… ì¶œë ¥
        print(f"ì˜ˆì¸¡ í™•ë¥ : {model.predict_proba([instance])[0]}")
        print(f"ì‹¤ì œ í´ë˜ìŠ¤: {y_train.iloc[instance_idx] if instance_idx < len(y_train) else 'Unknown'}")

        # LIME ê²°ê³¼ë¥¼ matplotlibìœ¼ë¡œ ì‹œê°í™”
        fig = explanation.as_pyplot_figure()
        fig.suptitle(f'{model_name} - LIME Explanation (Instance {instance_idx})')
        plt.tight_layout()
        plt.show()

        # í…ìŠ¤íŠ¸ ì„¤ëª…ë„ ì¶œë ¥
        print("\nLIME ì„¤ëª…:")
        for feature, weight in explanation.as_list():
            print(f"  {feature}: {weight:.4f}")

        return explanation

    except Exception as e:
        print(f"LIME ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ({model_name}): {str(e)}")
        return None

# =============================================================================
# 7. ì¢…í•© ë¶„ì„ í•¨ìˆ˜
# =============================================================================
def comprehensive_explainability_analysis(file_path, target_models=None):
    """SHAPê³¼ LIMEì„ ì´ìš©í•œ ì¢…í•©ì ì¸ ì„¤ëª…ê°€ëŠ¥ì„± ë¶„ì„"""

    # ë°ì´í„° ë° ëª¨ë¸ ì¤€ë¹„
    trained_models, y_train, y_test, scaler = prepare_data_and_models(file_path)

    # ë¶„ì„í•  ëª¨ë¸ ì„ íƒ
    if target_models is None:
        target_models = ['RandomForest', 'GradientBoosting', 'LogisticRegression']

    results = {}

    for model_name in target_models:
        if model_name not in trained_models:
            print(f"ëª¨ë¸ {model_name}ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            continue

        print(f"\n{'='*60}")
        print(f"ëª¨ë¸ ë¶„ì„: {model_name}")
        print(f"{'='*60}")

        model_info = trained_models[model_name]
        model = model_info['model']
        X_train = model_info['X_train']
        X_test = model_info['X_test']

        # ëª¨ë¸ ì„±ëŠ¥ ì¶œë ¥
        y_pred = model.predict(X_test)
        y_proba = model.predict_proba(X_test)[:, 1]

        print(f"\nëª¨ë¸ ì„±ëŠ¥:")
        print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
        print(f"ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}")

        # 1. SHAP Summary Plot
        explainer, shap_values = shap_summary_plot(model, model_name, X_test)

        if explainer is not None and shap_values is not None:
            # 2. SHAP Force Plot (ì²« ë²ˆì§¸ ì¸ìŠ¤í„´ìŠ¤)
            shap_force_plot(explainer, shap_values, X_test, model_name, instance_idx=0)

            # 3. SHAP Dependency Plot
            important_feature = shap_dependency_plot(explainer, shap_values, X_test, model_name)

            # 4. SHAP Feature Importance
            importance_df = shap_feature_importance(shap_values, X_test, model_name)

            results[model_name] = {
                'explainer': explainer,
                'shap_values': shap_values,
                'important_feature': important_feature,
                'feature_importance': importance_df
            }

        # 5. LIME Analysis (ì²« ë²ˆì§¸ì™€ ë‘ ë²ˆì§¸ ì¸ìŠ¤í„´ìŠ¤)
        for idx in [0, 1]:
            if idx < len(X_test):
                lime_explanation = lime_analysis(model, X_train, X_test, y_test, model_name,
                                               instance_idx=idx, num_features=8)

    return results, trained_models

# =============================================================================
# 8. ë¹„êµ ë¶„ì„ í•¨ìˆ˜
# =============================================================================
def compare_feature_importance_across_models(results):
    """ì—¬ëŸ¬ ëª¨ë¸ì˜ íŠ¹ì„± ì¤‘ìš”ë„ ë¹„êµ"""
    print(f"\n{'='*60}")
    print("ëª¨ë¸ë³„ íŠ¹ì„± ì¤‘ìš”ë„ ë¹„êµ")
    print(f"{'='*60}")

    # ëª¨ë“  ëª¨ë¸ì˜ íŠ¹ì„± ì¤‘ìš”ë„ë¥¼ í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ í•©ì¹˜ê¸°
    all_importance = []

    for model_name, result in results.items():
        if 'feature_importance' in result and result['feature_importance'] is not None:
            importance_df = result['feature_importance'].copy()
            importance_df['Model'] = model_name
            all_importance.append(importance_df)

    if all_importance:
        combined_df = pd.concat(all_importance, ignore_index=True)

        # í”¼ë²— í…Œì´ë¸” ìƒì„±
        pivot_df = combined_df.pivot(index='Feature', columns='Model', values='Importance')

        # íˆíŠ¸ë§µìœ¼ë¡œ ì‹œê°í™”
        plt.figure(figsize=(12, 10))
        sns.heatmap(pivot_df, annot=True, cmap='YlOrRd', fmt='.3f')
        plt.title('ëª¨ë¸ë³„ SHAP íŠ¹ì„± ì¤‘ìš”ë„ ë¹„êµ')
        plt.xlabel('ëª¨ë¸')
        plt.ylabel('íŠ¹ì„±')
        plt.tight_layout()
        plt.show()

        return pivot_df

    return None

# =============================================================================
# 9. ì‹¤í–‰ ì˜ˆì‹œ
# =============================================================================
if __name__ == "__main__":
    # íŒŒì¼ ê²½ë¡œ ì„¤ì •
    file_path = "/content/sample_data/bt_dataset_t3.csv"  # ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¡œ ë³€ê²½

    # ë¶„ì„í•  ëª¨ë¸ ì„ íƒ (Noneì´ë©´ ê¸°ë³¸ 3ê°œ ëª¨ë¸)
    target_models = ['RandomForest', 'GradientBoosting', 'LogisticRegression', 'SVM']

    # ì¢…í•© ë¶„ì„ ì‹¤í–‰
    results, trained_models = comprehensive_explainability_analysis(file_path, target_models)

    # ëª¨ë¸ê°„ íŠ¹ì„± ì¤‘ìš”ë„ ë¹„êµ
    comparison_df = compare_feature_importance_across_models(results)

    print("\n=== ë¶„ì„ ì™„ë£Œ ===")
    print("ëª¨ë“  SHAP ë° LIME ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

    # ê²°ê³¼ ìš”ì•½
    print(f"\në¶„ì„ëœ ëª¨ë¸ ìˆ˜: {len(results)}")
    print(f"ë¶„ì„ëœ ëª¨ë¸: {list(results.keys())}")



pip install streamlit

import streamlit as st
import os

st.set_page_config(page_title="SHAP & LIME í•´ì„", layout="wide")

st.title("ğŸ§¬ SHAP & LIME ê¸°ë°˜ ì„¤ëª…ê°€ëŠ¥ì„± ë¶„ì„ ì›¹ì•±")

uploaded_file = st.file_uploader("ğŸ“‚ CSV íŒŒì¼ ì—…ë¡œë“œ", type=["csv"])

if uploaded_file:
    file_path = "uploaded_dataset.csv"
    with open(file_path, "wb") as f:
        f.write(uploaded_file.read())

    st.success("âœ… íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ")

    # ì‚¬ìš©ìì—ê²Œ ë¶„ì„í•  ëª¨ë¸ ì„ íƒ ì˜µì…˜ ì œê³µ
    target_models = st.multiselect(
        "ğŸ§  ë¶„ì„í•  ëª¨ë¸ ì„ íƒ",
        ['RandomForest', 'GradientBoosting', 'LogisticRegression', 'SVM', 'KNN', 'AdaBoost'],
        default=['RandomForest', 'GradientBoosting', 'LogisticRegression']
    )

    if st.button("ğŸ” ì„¤ëª…ê°€ëŠ¥ì„± ë¶„ì„ ì‹¤í–‰"):
        with st.spinner("ëª¨ë¸ í›ˆë ¨ ë° SHAP & LIME ë¶„ì„ ì¤‘..."):
            results, trained_models = comprehensive_explainability_analysis(file_path, target_models)
            st.success("âœ… ë¶„ì„ ì™„ë£Œ")

            # ëª¨ë¸ë³„ ê²°ê³¼ ì‹œê°í™”
            for model_name, result in results.items():
                st.header(f"ğŸ“Š {model_name} ê²°ê³¼")

                # SHAP summary
                st.subheader("1ï¸âƒ£ SHAP Summary Plot")
                st.pyplot(result['shap_summary'])

                # SHAP dependency
                st.subheader("2ï¸âƒ£ SHAP Dependency Plot")
                st.pyplot(result['shap_depend'])

                # SHAP force plot
                st.subheader("3ï¸âƒ£ SHAP Force Plot (1ë²ˆì§¸ ìƒ˜í”Œ ê¸°ì¤€)")
                st.components.v1.html(result['shap_force'], height=300)

                # LIME explanation
                st.subheader("4ï¸âƒ£ LIME Explanation")
                st.plotly_chart(result['lime_plot'])

            # ì¤‘ìš”ë„ ë¹„êµ íˆíŠ¸ë§µ
            st.header("ğŸ“Œ ëª¨ë¸ ê°„ íŠ¹ì„± ì¤‘ìš”ë„ ë¹„êµ (SHAP ê¸°ì¤€)")
            comparison_df = compare_feature_importance_across_models(results)
            if comparison_df is not None:
                st.dataframe(comparison_df.round(3))
            else:
                st.warning("ì¤‘ìš”ë„ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# subgroup_analysis
import os
import pandas as pd
import numpy as np
from scipy.stats import ttest_ind, chi2_contingency

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1) ë°ì´í„° ë¡œë“œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
data_path = "/content/sample_data/bt_dataset_t3.csv"
df = pd.read_csv(data_path)

print(f"âœ” Data shape: {df.shape}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2) ê·¸ë£¹ ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
group_col = "Target"
grp0_val, grp1_val = 0, 1
grp0 = df[df[group_col] == grp0_val]
grp1 = df[df[group_col] == grp1_val]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3â€‘A) ìˆ˜ì¹˜í˜• Welch tâ€‘test
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
num_cols = [col for col in num_cols if col != group_col]

t_records = []
for col in num_cols:
    a, b = grp0[col].dropna(), grp1[col].dropna()
    if len(a) > 1 and len(b) > 1 and not (np.isclose(a.var(), 0) and np.isclose(b.var(), 0)):
        stat, p = ttest_ind(a, b, equal_var=False)
        t_records.append({"Variable": col, "T-stat": round(stat, 4), "P-value": round(p, 4)})

t_df = pd.DataFrame(t_records).sort_values("P-value")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3â€‘B) ë²”ì£¼í˜• Ï‡Â²
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cat_cols = df.select_dtypes(include=["object", "category", "bool"]).columns.tolist()

chi_records = []
for col in cat_cols:
    ct = pd.crosstab(df[col], df[group_col])
    if ct.shape[0] >= 2 and ct.shape[1] == 2:
        chi2, p, _, _ = chi2_contingency(ct)
        chi_records.append({"Variable": col, "Chi2": round(chi2, 4), "P-value": round(p, 4)})

chi_df = pd.DataFrame(chi_records).sort_values("P-value")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4) ì €ì¥
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
out_dir = "/content/drive/MyDrive/Colab Notebooks/output/subgroup_analysis"
os.makedirs(out_dir, exist_ok=True)
t_df.to_csv(f"{out_dir}/ttest_results.csv", index=False, encoding="utf-8-sig")
chi_df.to_csv(f"{out_dir}/chi2_results.csv", index=False, encoding="utf-8-sig")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5) ìš”ì•½ í…Œì´ë¸”
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
summary = pd.DataFrame([
    {"Analysis Type": "T-test",
     "Statistic": t_df["T-stat"].mean() if not t_df.empty else np.nan,
     "P-value": f"{t_df['P-value'].mean():.4e}" if not t_df.empty else "NA"},
    {"Analysis Type": "Chi-squared test",
     "Statistic": chi_df["Chi2"].mean() if not chi_df.empty else np.nan,
     "P-value": f"{chi_df['P-value'].mean():.4e}" if not chi_df.empty else "NA"}
])

summary.to_csv(f"{out_dir}/summary.csv", index=False, encoding="utf-8-sig")

print("\n=== Summary ===")
print(summary.to_string(index=False))
print("\n Files saved in:", out_dir)

# -*- coding: utf-8 -*-
"""bio_app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17mWKwYBmZH23aJjD6NfabGfwkRJBwuHI
"""

# 생물 데이터 수집
from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import StandardScaler
from matplotlib import pyplot as plt



# Tableone 이용 데이터 출력
pip install tableone

try:
    from tableone import TableOne, load_dataset
except (ModuleNotFoundError, ImportError):

    !pip install tableone
    from tableone import TableOne, load_dataset

data = pd.read_csv('/content/sample_data/bt_dataset_t3.csv')

table4_train = TableOne(data, groupby='Target', pval=True)
print(table4_train)
table4_train.to_csv('./Descriptive_Target.csv')

print(data.columns)
correlation=[]
for j in ['Mean', 'Variance', 'Standard Deviation', 'Entropy', 'Skewness', 'Kurtosis',
          'Contrast', 'Energy', 'ASM', 'Homogeneity', 'Dissimilarity', 'Correlation', 'Coarseness']:

  for i in ['Target']:
    print(j,i)
    X_train_total_del_week=data[[j]]
    X_train_total_del_week_remove = data[[i]]

!pip install umap-learn

from sklearn.manifold import TSNE
import umap
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns

features = ['Mean', 'Variance', 'Standard Deviation', 'Entropy', 'Skewness', 'Kurtosis',
            'Contrast', 'Energy', 'ASM', 'Homogeneity', 'Dissimilarity', 'Correlation','Coarseness']

X = data[features]
y = data['Target']

if X.isnull().sum().sum() > 0:
    print("결측치가 있어 제거합니다.")
    X = X.dropna()
    y = y[X.index]

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)



# t-SNE
tsne = TSNE(n_components=2, perplexity=30, random_state=42)
X_tsne = tsne.fit_transform(X_scaled)

# UMAP
reducer = umap.UMAP(n_components=2, random_state=42)
X_umap = reducer.fit_transform(X_scaled)

# t-SNE 시각화
plt.figure(figsize=(8,6))
sns.scatterplot(x=X_tsne[:,0], y=X_tsne[:,1], hue=y, palette='Set1', alpha=0.7)
plt.title('t-SNE Visualization')
plt.xlabel('t-SNE 1')
plt.ylabel('t-SNE 2')
plt.legend(title='Target')
plt.grid(True)
plt.show()

# u-MAP 시각화
plt.figure(figsize=(8,6))
sns.scatterplot(x=X_umap[:,0], y=X_umap[:,1], hue=y, palette='Set2', alpha=0.7)
plt.title('UMAP Visualization')
plt.xlabel('UMAP 1')
plt.ylabel('UMAP 2')
plt.legend(title='Target')
plt.grid(True)
plt.show()

# CCA
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.cross_decomposition import CCA
from scipy.stats import pearsonr, spearmanr
import warnings

warnings.filterwarnings("ignore")

# CSV 파일 경로 설정 (업로드된 파일 기준)
file_path = "/content/sample_data/bt_dataset_t3.csv"

# 데이터 불러오기
data = pd.read_csv(file_path)

# 타겟 컬럼 자동 탐색
target_col_candidates = [col for col in data.columns if col.lower() == 'target']
if not target_col_candidates:
    raise ValueError("'target' 컬럼이 데이터에 없습니다.")
else:
    target_col = target_col_candidates[0]  # 정확히는 'Target'

# 사용할 피처들 (12가지 특징)
selected_features = [
    'Mean', 'Variance', 'Standard Deviation', 'Entropy',
    'Skewness', 'Kurtosis', 'Contrast', 'Energy', 'ASM',
    'Homogeneity', 'Dissimilarity', 'Correlation'
]

# 결과 저장용 리스트
results = []

# 시각화 준비
fig, axs = plt.subplots(3, 4, figsize=(20, 15))
axs = axs.flatten()

# 각 특징별로 Canonical Correlation 계산 및 시각화
for idx, feature in enumerate(selected_features):
    try:
        X1 = data[data[target_col] == 0][[feature]].dropna()
        X2 = data[data[target_col] == 1][[feature]].dropna()

        # 두 그룹에서 동일한 개수만큼 샘플 유지 (min 기준)
        min_len = min(len(X1), len(X2))
        X1 = X1.iloc[:min_len]
        X2 = X2.iloc[:min_len]

        # CCA 적용
        cca = CCA(n_components=1)
        X1_c, X2_c = cca.fit_transform(X1, X2)

        # 상관계수 계산
        r_pearson, _ = pearsonr(X1_c[:, 0], X2_c[:, 0])
        r_spearman, _ = spearmanr(X1_c[:, 0], X2_c[:, 0])

        results.append({
            'Feature': feature,
            'Pearson r': r_pearson,
            'Spearman r': r_spearman
        })

        # 산점도 그래프
        ax = axs[idx]
        ax.scatter(X1_c[:, 0], X2_c[:, 0], alpha=0.7)
        ax.set_title(f"{feature}\nPearson r={r_pearson:.2f}, Spearman r={r_spearman:.2f}")
        ax.set_xlabel("Non-Tumor (Group 0)")
        ax.set_ylabel("Tumor (Group 1)")

    except Exception as e:
        print(f"[ERROR] {feature} - CCA 계산 중 오류 발생: {e}")

# 전체 그래프 레이아웃 조정 및 출력
plt.tight_layout()
plt.suptitle("Canonical Correlation Analysis (Group 0 vs. Group 1)", fontsize=16, y=1.02)
plt.show()

# 결과 DataFrame으로 정리
results_df = pd.DataFrame(results)
results_df

#머신러닝 성능 비교(5종)
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier

# 데이터 불러오기
data = pd.read_csv('/content/sample_data/bt_dataset_t3.csv')

features = ['Mean', 'Variance', 'Standard Deviation', 'Entropy', 'Skewness', 'Kurtosis',
            'Contrast', 'Energy', 'ASM', 'Homogeneity', 'Dissimilarity', 'Correlation','Coarseness']
X = data[features]
y = data['Target']

# 결측치 제거
X = X.dropna()
y = y[X.index]

# 데이터 분할
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# 스케일링
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 모델 리스트
models = {
    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
    'SVM': SVC(kernel='rbf', probability=True, random_state=42),
    'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),
    'KNN': KNeighborsClassifier(n_neighbors=5),
    'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42)
}

results_ml = []

for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    y_proba = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, 'predict_proba') else None
    acc = accuracy_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_proba) if y_proba is not None else np.nan
    print(f"\n[{name}]")
    print(classification_report(y_test, y_pred))
    print(f"Accuracy: {acc:.4f}, ROC-AUC: {auc:.4f}")
    results_ml.append({'Model': name, 'Accuracy': acc, 'ROC-AUC': auc})

results_ml_df = pd.DataFrame(results_ml)
print("\n머신러닝 모델 성능 비교:")
print(results_ml_df)



#딥러닝 모델 성능 비교(2종)
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# 딥러닝용 데이터 (이미 스케일링됨)
X_train_dl, X_test_dl = X_train_scaled, X_test_scaled
y_train_dl, y_test_dl = y_train.values, y_test.values

# MLP 모델
def build_mlp(input_dim):
    model = keras.Sequential([
        layers.Dense(64, activation='relu', input_shape=(input_dim,)),
        layers.Dense(32, activation='relu'),
        layers.Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])
    return model

# 1D CNN 모델
def build_cnn(input_dim):
    model = keras.Sequential([
        layers.Reshape((input_dim, 1), input_shape=(input_dim,)),
        layers.Conv1D(32, 3, activation='relu'),
        layers.Flatten(),
        layers.Dense(32, activation='relu'),
        layers.Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])
    return model

# 학습 및 평가 함수
def train_and_evaluate(model, X_train, y_train, X_test, y_test, epochs=30, batch_size=32):
    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0, validation_split=0.2)
    loss, acc, auc = model.evaluate(X_test, y_test, verbose=0)
    return acc, auc

dl_results = []

# 1. MLP
mlp = build_mlp(X_train_dl.shape[1])
mlp_acc, mlp_auc = train_and_evaluate(mlp, X_train_dl, y_train_dl, X_test_dl, y_test_dl)
dl_results.append({'Model': 'MLP', 'Accuracy': mlp_acc, 'ROC-AUC': mlp_auc})

# 2. 1D CNN
cnn = build_cnn(X_train_dl.shape[1])
cnn_acc, cnn_auc = train_and_evaluate(cnn, X_train_dl, y_train_dl, X_test_dl, y_test_dl)
dl_results.append({'Model': '1D CNN', 'Accuracy': cnn_acc, 'ROC-AUC': cnn_auc})

dl_results_df = pd.DataFrame(dl_results)
print("\n딥러닝 모델 성능 비교:")
print(dl_results_df)

all_results = pd.concat([results_ml_df, dl_results_df], ignore_index=True)
print("\n전체 모델 성능 비교:")
print(all_results)

# Odd ratio
import pandas as pd
import numpy as np
import statsmodels.api as sm
import scipy.stats as stats

# 1. 데이터 로딩
data = pd.read_csv('/content/sample_data/bt_dataset_t3.csv')

# 2. 사용할 feature 및 종속 변수 설정
features = ['Mean', 'Variance', 'Standard Deviation', 'Entropy', 'Skewness', 'Kurtosis',
            'Contrast', 'Energy', 'ASM', 'Homogeneity', 'Dissimilarity', 'Correlation', 'Coarseness']
target = 'Target'

# 3. Odds Ratio 계산 함수
def odd_ratio(data, features, target):
    print('='*70)
    print(f'{"Feature":<20} {"Odds ratio (CI)":<30} {"p-value":<10}')
    print('-'*70)

    for feature in features:
        try:
            X = sm.add_constant(data[feature])
            y = data[target]

            # Poisson 회귀로 odds ratio 추정
            model = sm.GLM(y, X, family=sm.families.Poisson()).fit()

            coef = model.params[1]
            odds_ratio = np.exp(coef)
            CI = model.conf_int().iloc[1]
            CI_low = np.round(np.exp(CI[0]), 3)
            CI_high = np.round(np.exp(CI[1]), 3)
            p_value = model.pvalues[1]

            print(f'{feature:<20} {odds_ratio:.3f} ({CI_low:.3f} - {CI_high:.3f})   {p_value:.4f}')
        except Exception as e:
            print(f'{feature:<20} 계산 실패 - {e}')

    print('='*70)


# 4. t-test 계산 함수
def make_ttest(data, features, target):
    print('='*70)
    print(f'{"Feature":<20} {"t-statistic":<15} {"p-value":<10}')
    print('-'*70)

    for feature in features:
        try:
            group1 = data[data[target] == 0][feature]
            group2 = data[data[target] == 1][feature]

            t_stat, p_value = stats.ttest_ind(group1, group2, nan_policy='omit')
            print(f'{feature:<20} {t_stat:.3f}         {p_value:.4f}')
        except Exception as e:
            print(f'{feature:<20} 계산 실패 - {e}')

    print('='*70)

# 5. 실행
odd_ratio(data, features, target)
make_ttest(data, features, target)

# SHAP & LIME 분석을 위한 종합 코드
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")

# SHAP 및 LIME 라이브러리
try:
    import shap
    shap.initjs()
except ImportError:
    print("SHAP 라이브러리를 설치해주세요: pip install shap")

try:
    import lime
    import lime.lime_tabular
except ImportError:
    print("LIME 라이브러리를 설치해주세요: pip install lime")

# 머신러닝 라이브러리
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report

# =============================================================================
# 1. 데이터 준비 및 모델 학습
# =============================================================================
def prepare_data_and_models(file_path):
    """데이터 준비 및 여러 모델 학습"""
    # 데이터 로딩
    data = pd.read_csv(file_path)

    features = ['Mean', 'Variance', 'Standard Deviation', 'Entropy', 'Skewness', 'Kurtosis',
                'Contrast', 'Energy', 'ASM', 'Homogeneity', 'Dissimilarity', 'Correlation', 'Coarseness']

    X = data[features].dropna()
    y = data.loc[X.index, 'Target']

    # 데이터 분할
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

    # 스케일링 (일부 모델용)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # 스케일된 데이터를 DataFrame으로 변환 (feature names 유지)
    X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=features, index=X_train.index)
    X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=features, index=X_test.index)

    # 모델 딕셔너리
    models = {
        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
        'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
        'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),
        'SVM': SVC(kernel='rbf', probability=True, random_state=42),
        'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=42),
        'KNN': KNeighborsClassifier(n_neighbors=5)
    }

    # 모델 학습
    trained_models = {}
    for name, model in models.items():
        if name in ['LogisticRegression', 'SVM', 'KNN']:
            # 스케일된 데이터 사용
            model.fit(X_train_scaled, y_train)
            trained_models[name] = {
                'model': model,
                'X_train': X_train_scaled_df,
                'X_test': X_test_scaled_df,
                'scaled': True
            }
        else:
            # 원본 데이터 사용
            model.fit(X_train, y_train)
            trained_models[name] = {
                'model': model,
                'X_train': X_train,
                'X_test': X_test,
                'scaled': False
            }

    return trained_models, y_train, y_test, scaler

# =============================================================================
# 2. SHAP Summary Plot
# =============================================================================
def shap_summary_plot(model, model_name, X, model_type='auto'):
    """SHAP Summary Plot 생성"""
    print(f'\n=== {model_name} - SHAP Summary Plot ===')

    try:
        # 모델 타입에 따른 explainer 선택
        if isinstance(model, (RandomForestClassifier, GradientBoostingClassifier)):
            explainer = shap.TreeExplainer(model)
            shap_values = explainer.shap_values(X)

            # 이진 분류의 경우 positive class의 SHAP values 사용
            if isinstance(shap_values, list) and len(shap_values) == 2:
                shap_values = shap_values[1]

        elif isinstance(model, (AdaBoostClassifier, BaggingClassifier, SGDClassifier, SVC, KNeighborsClassifier)):
            # 샘플 크기 제한 (KernelExplainer는 느림)
            sample_size = min(100, len(X))
            X_sample = X.sample(n=sample_size, random_state=42) if len(X) > sample_size else X

            explainer = shap.KernelExplainer(model.predict_proba, X_sample)
            shap_values = explainer.shap_values(X_sample)

            if isinstance(shap_values, list) and len(shap_values) == 2:
                shap_values = shap_values[1]
            X = X_sample

        else:  # LogisticRegression 등
            explainer = shap.LinearExplainer(model, X)
            shap_values = explainer.shap_values(X)

        # Summary plot 생성
        plt.figure(figsize=(10, 8))
        shap.summary_plot(shap_values, X, show=False)
        plt.title(f'{model_name} - SHAP Summary Plot')
        plt.tight_layout()
        plt.show()

        return explainer, shap_values

    except Exception as e:
        print(f"SHAP Summary Plot 생성 중 오류 ({model_name}): {str(e)}")
        return None, None

# =============================================================================
# 3. SHAP Force Plot
# =============================================================================
def shap_force_plot(explainer, shap_values, X, model_name, instance_idx=0):
    """SHAP Force Plot 생성"""
    print(f'\n=== {model_name} - SHAP Force Plot (Instance {instance_idx}) ===')

    try:
        # expected_value 처리
        if hasattr(explainer, 'expected_value'):
            if isinstance(explainer.expected_value, np.ndarray):
                expected_value = explainer.expected_value[1] if len(explainer.expected_value) > 1 else explainer.expected_value[0]
            else:
                expected_value = explainer.expected_value
        else:
            expected_value = 0

        # SHAP values 처리
        if len(shap_values.shape) == 2:
            instance_shap = shap_values[instance_idx, :]
        else:
            instance_shap = shap_values[instance_idx]

        # Force plot 생성 (matplotlib 버전)
        plt.figure(figsize=(12, 4))
        shap.force_plot(
            expected_value,
            instance_shap,
            X.iloc[instance_idx, :],
            matplotlib=True,
            show=False
        )
        plt.title(f'{model_name} - SHAP Force Plot (Instance {instance_idx})')
        plt.tight_layout()
        plt.show()

    except Exception as e:
        print(f"SHAP Force Plot 생성 중 오류 ({model_name}): {str(e)}")

# =============================================================================
# 4. SHAP Dependency Plot
# =============================================================================
def shap_dependency_plot(explainer, shap_values, X, model_name, feature=None):
    """SHAP Dependency Plot 생성"""
    print(f'\n=== {model_name} - SHAP Dependency Plot ===')

    try:
        # 가장 중요한 특성 찾기 (feature가 지정되지 않은 경우)
        if feature is None:
            mean_abs_shap = np.abs(shap_values).mean(axis=0)
            most_important_idx = np.argmax(mean_abs_shap)
            feature = X.columns[most_important_idx]

        print(f'분석 특성: {feature}')

        # Dependency plot 생성
        plt.figure(figsize=(10, 6))
        shap.dependence_plot(feature, shap_values, X, show=False)
        plt.title(f'{model_name} - SHAP Dependency Plot ({feature})')
        plt.tight_layout()
        plt.show()

        return feature

    except Exception as e:
        print(f"SHAP Dependency Plot 생성 중 오류 ({model_name}): {str(e)}")
        return None

# =============================================================================
# 5. SHAP Feature Importance
# =============================================================================
def shap_feature_importance(shap_values, X, model_name, top_n=10):
    """SHAP 기반 특성 중요도 분석"""
    print(f'\n=== {model_name} - SHAP Feature Importance ===')

    try:
        # 평균 절댓값으로 특성 중요도 계산
        feature_importance = np.abs(shap_values).mean(axis=0)

        # DataFrame으로 정리
        importance_df = pd.DataFrame({
            'Feature': X.columns,
            'Importance': feature_importance
        }).sort_values('Importance', ascending=False)

        # 시각화
        plt.figure(figsize=(10, 8))
        sns.barplot(data=importance_df.head(top_n), y='Feature', x='Importance')
        plt.title(f'{model_name} - SHAP Feature Importance (Top {top_n})')
        plt.xlabel('Mean |SHAP Value|')
        plt.tight_layout()
        plt.show()

        print(f"Top {top_n} 중요 특성:")
        print(importance_df.head(top_n))

        return importance_df

    except Exception as e:
        print(f"SHAP Feature Importance 분석 중 오류 ({model_name}): {str(e)}")
        return None

# =============================================================================
# 6. LIME 분석
# =============================================================================
def lime_analysis(model, X_train, X_test, y_train, model_name, instance_idx=0, num_features=10):
    """LIME 분석"""
    print(f'\n=== {model_name} - LIME Analysis (Instance {instance_idx}) ===')

    try:
        # LIME explainer 생성
        explainer = lime.lime_tabular.LimeTabularExplainer(
            X_train.values,
            feature_names=X_train.columns,
            class_names=['Non-Tumor', 'Tumor'],
            mode='classification',
            discretize_continuous=True
        )

        # 특정 인스턴스에 대한 설명 생성
        instance = X_test.iloc[instance_idx].values
        explanation = explainer.explain_instance(
            instance,
            model.predict_proba,
            num_features=num_features
        )

        # 설명 출력
        print(f"예측 확률: {model.predict_proba([instance])[0]}")
        print(f"실제 클래스: {y_train.iloc[instance_idx] if instance_idx < len(y_train) else 'Unknown'}")

        # LIME 결과를 matplotlib으로 시각화
        fig = explanation.as_pyplot_figure()
        fig.suptitle(f'{model_name} - LIME Explanation (Instance {instance_idx})')
        plt.tight_layout()
        plt.show()

        # 텍스트 설명도 출력
        print("\nLIME 설명:")
        for feature, weight in explanation.as_list():
            print(f"  {feature}: {weight:.4f}")

        return explanation

    except Exception as e:
        print(f"LIME 분석 중 오류 ({model_name}): {str(e)}")
        return None

# =============================================================================
# 7. 종합 분석 함수
# =============================================================================
def comprehensive_explainability_analysis(file_path, target_models=None):
    """SHAP과 LIME을 이용한 종합적인 설명가능성 분석"""

    # 데이터 및 모델 준비
    trained_models, y_train, y_test, scaler = prepare_data_and_models(file_path)

    # 분석할 모델 선택
    if target_models is None:
        target_models = ['RandomForest', 'GradientBoosting', 'LogisticRegression']

    results = {}

    for model_name in target_models:
        if model_name not in trained_models:
            print(f"모델 {model_name}을 찾을 수 없습니다.")
            continue

        print(f"\n{'='*60}")
        print(f"모델 분석: {model_name}")
        print(f"{'='*60}")

        model_info = trained_models[model_name]
        model = model_info['model']
        X_train = model_info['X_train']
        X_test = model_info['X_test']

        # 모델 성능 출력
        y_pred = model.predict(X_test)
        y_proba = model.predict_proba(X_test)[:, 1]

        print(f"\n모델 성능:")
        print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
        print(f"ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}")

        # 1. SHAP Summary Plot
        explainer, shap_values = shap_summary_plot(model, model_name, X_test)

        if explainer is not None and shap_values is not None:
            # 2. SHAP Force Plot (첫 번째 인스턴스)
            shap_force_plot(explainer, shap_values, X_test, model_name, instance_idx=0)

            # 3. SHAP Dependency Plot
            important_feature = shap_dependency_plot(explainer, shap_values, X_test, model_name)

            # 4. SHAP Feature Importance
            importance_df = shap_feature_importance(shap_values, X_test, model_name)

            results[model_name] = {
                'explainer': explainer,
                'shap_values': shap_values,
                'important_feature': important_feature,
                'feature_importance': importance_df
            }

        # 5. LIME Analysis (첫 번째와 두 번째 인스턴스)
        for idx in [0, 1]:
            if idx < len(X_test):
                lime_explanation = lime_analysis(model, X_train, X_test, y_test, model_name,
                                               instance_idx=idx, num_features=8)

    return results, trained_models

# =============================================================================
# 8. 비교 분석 함수
# =============================================================================
def compare_feature_importance_across_models(results):
    """여러 모델의 특성 중요도 비교"""
    print(f"\n{'='*60}")
    print("모델별 특성 중요도 비교")
    print(f"{'='*60}")

    # 모든 모델의 특성 중요도를 하나의 DataFrame으로 합치기
    all_importance = []

    for model_name, result in results.items():
        if 'feature_importance' in result and result['feature_importance'] is not None:
            importance_df = result['feature_importance'].copy()
            importance_df['Model'] = model_name
            all_importance.append(importance_df)

    if all_importance:
        combined_df = pd.concat(all_importance, ignore_index=True)

        # 피벗 테이블 생성
        pivot_df = combined_df.pivot(index='Feature', columns='Model', values='Importance')

        # 히트맵으로 시각화
        plt.figure(figsize=(12, 10))
        sns.heatmap(pivot_df, annot=True, cmap='YlOrRd', fmt='.3f')
        plt.title('모델별 SHAP 특성 중요도 비교')
        plt.xlabel('모델')
        plt.ylabel('특성')
        plt.tight_layout()
        plt.show()

        return pivot_df

    return None

# =============================================================================
# 9. 실행 예시
# =============================================================================
if __name__ == "__main__":
    # 파일 경로 설정
    file_path = "/content/sample_data/bt_dataset_t3.csv"  # 실제 파일 경로로 변경

    # 분석할 모델 선택 (None이면 기본 3개 모델)
    target_models = ['RandomForest', 'GradientBoosting', 'LogisticRegression', 'SVM']

    # 종합 분석 실행
    results, trained_models = comprehensive_explainability_analysis(file_path, target_models)

    # 모델간 특성 중요도 비교
    comparison_df = compare_feature_importance_across_models(results)

    print("\n=== 분석 완료 ===")
    print("모든 SHAP 및 LIME 분석이 완료되었습니다.")

    # 결과 요약
    print(f"\n분석된 모델 수: {len(results)}")
    print(f"분석된 모델: {list(results.keys())}")



pip install streamlit

import streamlit as st
import os

st.set_page_config(page_title="SHAP & LIME 해석", layout="wide")

st.title("🧬 SHAP & LIME 기반 설명가능성 분석 웹앱")

uploaded_file = st.file_uploader("📂 CSV 파일 업로드", type=["csv"])

if uploaded_file:
    file_path = "uploaded_dataset.csv"
    with open(file_path, "wb") as f:
        f.write(uploaded_file.read())

    st.success("✅ 파일 업로드 완료")

    # 사용자에게 분석할 모델 선택 옵션 제공
    target_models = st.multiselect(
        "🧠 분석할 모델 선택",
        ['RandomForest', 'GradientBoosting', 'LogisticRegression', 'SVM', 'KNN', 'AdaBoost'],
        default=['RandomForest', 'GradientBoosting', 'LogisticRegression']
    )

    if st.button("🔍 설명가능성 분석 실행"):
        with st.spinner("모델 훈련 및 SHAP & LIME 분석 중..."):
            results, trained_models = comprehensive_explainability_analysis(file_path, target_models)
            st.success("✅ 분석 완료")

            # 모델별 결과 시각화
            for model_name, result in results.items():
                st.header(f"📊 {model_name} 결과")

                # SHAP summary
                st.subheader("1️⃣ SHAP Summary Plot")
                st.pyplot(result['shap_summary'])

                # SHAP dependency
                st.subheader("2️⃣ SHAP Dependency Plot")
                st.pyplot(result['shap_depend'])

                # SHAP force plot
                st.subheader("3️⃣ SHAP Force Plot (1번째 샘플 기준)")
                st.components.v1.html(result['shap_force'], height=300)

                # LIME explanation
                st.subheader("4️⃣ LIME Explanation")
                st.plotly_chart(result['lime_plot'])

            # 중요도 비교 히트맵
            st.header("📌 모델 간 특성 중요도 비교 (SHAP 기준)")
            comparison_df = compare_feature_importance_across_models(results)
            if comparison_df is not None:
                st.dataframe(comparison_df.round(3))
            else:
                st.warning("중요도 데이터를 불러올 수 없습니다.")

# subgroup_analysis
import os
import pandas as pd
import numpy as np
from scipy.stats import ttest_ind, chi2_contingency

# ───────────────────────────
# 1) 데이터 로드
# ───────────────────────────
data_path = "/content/sample_data/bt_dataset_t3.csv"
df = pd.read_csv(data_path)

print(f"✔ Data shape: {df.shape}")

# ───────────────────────────
# 2) 그룹 설정
# ───────────────────────────
group_col = "Target"
grp0_val, grp1_val = 0, 1
grp0 = df[df[group_col] == grp0_val]
grp1 = df[df[group_col] == grp1_val]

# ───────────────────────────
# 3‑A) 수치형 Welch t‑test
# ───────────────────────────
num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
num_cols = [col for col in num_cols if col != group_col]

t_records = []
for col in num_cols:
    a, b = grp0[col].dropna(), grp1[col].dropna()
    if len(a) > 1 and len(b) > 1 and not (np.isclose(a.var(), 0) and np.isclose(b.var(), 0)):
        stat, p = ttest_ind(a, b, equal_var=False)
        t_records.append({"Variable": col, "T-stat": round(stat, 4), "P-value": round(p, 4)})

t_df = pd.DataFrame(t_records).sort_values("P-value")

# ───────────────────────────
# 3‑B) 범주형 χ²
# ───────────────────────────
cat_cols = df.select_dtypes(include=["object", "category", "bool"]).columns.tolist()

chi_records = []
for col in cat_cols:
    ct = pd.crosstab(df[col], df[group_col])
    if ct.shape[0] >= 2 and ct.shape[1] == 2:
        chi2, p, _, _ = chi2_contingency(ct)
        chi_records.append({"Variable": col, "Chi2": round(chi2, 4), "P-value": round(p, 4)})

chi_df = pd.DataFrame(chi_records).sort_values("P-value")

# ───────────────────────────
# 4) 저장
# ───────────────────────────
out_dir = "/content/drive/MyDrive/Colab Notebooks/output/subgroup_analysis"
os.makedirs(out_dir, exist_ok=True)
t_df.to_csv(f"{out_dir}/ttest_results.csv", index=False, encoding="utf-8-sig")
chi_df.to_csv(f"{out_dir}/chi2_results.csv", index=False, encoding="utf-8-sig")

# ───────────────────────────
# 5) 요약 테이블
# ───────────────────────────
summary = pd.DataFrame([
    {"Analysis Type": "T-test",
     "Statistic": t_df["T-stat"].mean() if not t_df.empty else np.nan,
     "P-value": f"{t_df['P-value'].mean():.4e}" if not t_df.empty else "NA"},
    {"Analysis Type": "Chi-squared test",
     "Statistic": chi_df["Chi2"].mean() if not chi_df.empty else np.nan,
     "P-value": f"{chi_df['P-value'].mean():.4e}" if not chi_df.empty else "NA"}
])

summary.to_csv(f"{out_dir}/summary.csv", index=False, encoding="utf-8-sig")

print("\n=== Summary ===")
print(summary.to_string(index=False))
print("\n Files saved in:", out_dir)
